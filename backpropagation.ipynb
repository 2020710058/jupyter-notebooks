{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "7 x 6 x 5 x 2 의 4계층 신경망 구조를 작성하여 오차 역전파로 학습이 잘 진행되고, 편미분이 수치 미분의 결과와 동일한지 검증한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "def init():\n",
    "    global i, w_1, b_1, w_2, b_2, w_3, b_3, t\n",
    "        \n",
    "    i = np.array([0.4,-0.2,0.1,0.1,-0.15,0.6,-0.9]).reshape(-1, 1)\n",
    "\n",
    "    np.random.seed(12)\n",
    "    w_1 = np.random.rand(6, 7)\n",
    "    b_1 = np.random.rand(6).reshape(-1, 1)\n",
    "    w_2 = np.random.rand(5, 6)\n",
    "    b_2 = np.random.rand(5).reshape(-1, 1)\n",
    "    w_3 = np.random.rand(2, 5)\n",
    "    b_3 = np.random.rand(2).reshape(-1, 1)\n",
    "    \n",
    "    t = np.array([[0.87503811],[0.83690408]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z: np.ndarray):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def d_sigmoid(a: np.ndarray):\n",
    "    return a * (1.0 - a)\n",
    "\n",
    "def gradient_check(analytic, numeric):\n",
    "    numerator = abs(analytic-numeric)\n",
    "    denominator = max(analytic,numeric)\n",
    "    difference = numerator/denominator\n",
    "    \n",
    "    if difference < 1e-3: # cs231n의 권장 수치는 1e-7인데 여기서는 좀 더 높인다.\n",
    "        print (\"The gradient is correct!\")\n",
    "    else:\n",
    "        print (\"The gradient is wrong!\")\n",
    "        \n",
    "def forward(i):\n",
    "    global w_1, b_1, w_2, b_2, w_3, b_3\n",
    "    \n",
    "    net_h1 = np.dot(w_1, i) + b_1\n",
    "    out_h1 = sigmoid(net_h1)\n",
    "    \n",
    "    net_h2 = np.dot(w_2, out_h1) + b_2\n",
    "    out_h2 = sigmoid(net_h2)\n",
    "    \n",
    "    net_o = np.dot(w_3, out_h2) + b_3\n",
    "    out_o = sigmoid(net_o)\n",
    "    \n",
    "    return net_h1, out_h1, net_h2, out_h2, net_o, out_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(i, t):\n",
    "    lr = 0.1\n",
    "    \n",
    "    global w_1, b_1, w_2, b_2, w_3, b_3\n",
    "    global delta_w_1, delta_b_1, delta_w_2, delta_b_2, delta_w_3, delta_b_3\n",
    "    \n",
    "    net_h1, out_h1, net_h2, out_h2, net_o, out_o = forward(i)\n",
    "\n",
    "    # backpropagation!\n",
    "    d_o_errors = - (t - out_o)\n",
    "    delta_w_3 = np.dot(d_o_errors * d_sigmoid(out_o), out_h2.T)\n",
    "    w_3 += - lr * delta_w_3\n",
    "    delta_b_3 = d_o_errors * d_sigmoid(out_o)\n",
    "    b_3 += - lr * delta_b_3\n",
    "\n",
    "    d_h2_errors = np.dot(w_3.T, d_o_errors * d_sigmoid(out_o))\n",
    "    delta_w_2 = np.dot(d_h2_errors * d_sigmoid(out_h2), out_h1.T)\n",
    "    w_2 += - lr * delta_w_2\n",
    "    delta_b_2 = d_h2_errors * d_sigmoid(out_h2)\n",
    "    b_2 += - lr * delta_b_2\n",
    "\n",
    "    d_h1_errors = np.dot(w_2.T, d_h2_errors * d_sigmoid(out_h2))\n",
    "    delta_w_1 = np.dot(d_h1_errors * d_sigmoid(out_h1), i.T)\n",
    "    w_1 += - lr * delta_w_1\n",
    "    delta_b_1 = d_h1_errors * d_sigmoid(out_h1)\n",
    "    b_1 += - lr * delta_b_1\n",
    "    \n",
    "\n",
    "def query(i, t):\n",
    "    _, _, _, _, _, out_o = forward(i)\n",
    "\n",
    "    # E 출력\n",
    "    print(t - out_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.09422695]\n",
      " [-0.12718581]]\n"
     ]
    }
   ],
   "source": [
    "init()\n",
    "train(i,t)\n",
    "query(i,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0000974372978874, 0.0000974974961876\n",
      "The gradient is correct!\n",
      "-0.0000487186489437, -0.0000487487654410\n",
      "The gradient is correct!\n",
      "0.0000243593244718, 0.0000243742439426\n",
      "The gradient is correct!\n",
      "0.0000243593244718, 0.0000243742439426\n",
      "The gradient is correct!\n",
      "-0.0000365389867078, -0.0000365616738274\n",
      "The gradient is correct!\n",
      "0.0001461559468311, 0.0001462461922397\n",
      "The gradient is correct!\n",
      "-0.0002192339202466, -0.0002193693057068\n",
      "The gradient is correct!\n",
      "\n",
      "0.0002435932447185, 0.0002437437317954\n",
      "The gradient is correct!\n",
      "\n",
      "0.0000913394440743, 0.0000913891792104\n",
      "The gradient is correct!\n",
      "-0.0000456697220372, -0.0000456946720045\n",
      "The gradient is correct!\n",
      "0.0000228348610186, 0.0000228473663599\n",
      "The gradient is correct!\n",
      "0.0000228348610186, 0.0000228473663599\n",
      "The gradient is correct!\n",
      "-0.0000342522915279, -0.0000342708066786\n",
      "The gradient is correct!\n",
      "0.0001370091661115, 0.0001370838078468\n",
      "The gradient is correct!\n",
      "-0.0002055137491673, -0.0002056257334543\n",
      "The gradient is correct!\n",
      "\n",
      "0.0002283486101858, 0.0002284729523627\n",
      "The gradient is correct!\n",
      "\n",
      "0.0000386111993653, 0.0000386394215235\n",
      "The gradient is correct!\n",
      "-0.0000193055996826, -0.0000193198061715\n",
      "The gradient is correct!\n",
      "0.0000096527998413, 0.0000096598336968\n",
      "The gradient is correct!\n",
      "0.0000096527998413, 0.0000096598336968\n",
      "The gradient is correct!\n",
      "-0.0000144791997620, -0.0000144898155974\n",
      "The gradient is correct!\n",
      "0.0000579167990479, 0.0000579590282018\n",
      "The gradient is correct!\n",
      "-0.0000868751985719, -0.0000869389196051\n",
      "The gradient is correct!\n",
      "\n",
      "0.0000965279984132, 0.0000965988053436\n",
      "The gradient is correct!\n",
      "\n",
      "0.0001209784503821, 0.0001210589180983\n",
      "The gradient is correct!\n",
      "-0.0000604892251910, -0.0000605292899136\n",
      "The gradient is correct!\n",
      "0.0000302446125955, 0.0000302647143458\n",
      "The gradient is correct!\n",
      "0.0000302446125955, 0.0000302647143458\n",
      "The gradient is correct!\n",
      "-0.0000453669188933, -0.0000453971756020\n",
      "The gradient is correct!\n",
      "0.0001814676755731, 0.0001815882947481\n",
      "The gradient is correct!\n",
      "-0.0002722015133596, -0.0002723824204381\n",
      "The gradient is correct!\n",
      "\n",
      "0.0003024461259551, 0.0003026473082562\n",
      "The gradient is correct!\n",
      "\n",
      "0.0000982174908258, 0.0000982817906919\n",
      "The gradient is correct!\n",
      "-0.0000491087454129, -0.0000491408823355\n",
      "The gradient is correct!\n",
      "0.0000245543727065, 0.0000245705279039\n",
      "The gradient is correct!\n",
      "0.0000245543727065, 0.0000245705279039\n",
      "The gradient is correct!\n",
      "-0.0000368315590597, -0.0000368554839425\n",
      "The gradient is correct!\n",
      "0.0001473262362388, 0.0001474226123122\n",
      "The gradient is correct!\n",
      "-0.0002209893543581, -0.0002211341353087\n",
      "The gradient is correct!\n",
      "\n",
      "0.0002455437270646, 0.0002457044376986\n",
      "The gradient is correct!\n",
      "\n",
      "0.0000614037023519, 0.0000614382329733\n",
      "The gradient is correct!\n",
      "-0.0000307018511759, -0.0000307189863824\n",
      "The gradient is correct!\n",
      "0.0000153509255880, 0.0000153595712538\n",
      "The gradient is correct!\n",
      "0.0000153509255880, 0.0000153595712538\n",
      "The gradient is correct!\n",
      "-0.0000230263883820, -0.0000230394262696\n",
      "The gradient is correct!\n",
      "0.0000921055535278, 0.0000921573494600\n",
      "The gradient is correct!\n",
      "-0.0001381583302918, -0.0001382358897489\n",
      "The gradient is correct!\n",
      "\n",
      "0.0001535092558797, 0.0001535954956972\n",
      "The gradient is correct!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gradient Checking(w_1, b_1)\n",
    "init()\n",
    "train(i,t)\n",
    "h = 1e-7\n",
    "\n",
    "init()\n",
    "for k in range(6):\n",
    "    for j in range(7):\n",
    "        # 수치 미분(numerical gradient) 진행\n",
    "        init()\n",
    "        w_1[k][j] += h\n",
    "        _, _, _, _, _, out_o = forward(i)\n",
    "        e1 = np.sum((t - out_o) ** 2) / 2\n",
    "\n",
    "        init()\n",
    "        w_1[k][j] -= h\n",
    "        _, _, _, _, _, out_o = forward(i)\n",
    "        e2 = np.sum((t - out_o) ** 2) / 2\n",
    "\n",
    "        # 수치 미분 결과가 해석적 미분(analytic gradient)과 동일한지 검증\n",
    "        numeric_delta_w_1 = (e1 - e2) / (2 * h)\n",
    "        print(\"%.16f, %.16f\" % (delta_w_1[k][j], numeric_delta_w_1))\n",
    "        gradient_check(delta_w_1[k][j], numeric_delta_w_1)\n",
    "\n",
    "    init()\n",
    "    b_1[k] += h\n",
    "    _, _, _, _, _, out_o = forward(i)\n",
    "    e1 = np.sum((t - out_o) ** 2) / 2\n",
    "\n",
    "    init()\n",
    "    b_1[k] -= h\n",
    "    _, _, _, _, _, out_o = forward(i)\n",
    "    e2 = np.sum((t - out_o) ** 2) / 2\n",
    "\n",
    "    # 수치 미분 결과가 해석적 미분(analytic gradient)과 동일한지 검증\n",
    "    numeric_delta_b_1 = (e1 - e2) / (2 * h)\n",
    "    print()\n",
    "    print(\"%.16f, %.16f\" % (delta_b_1[k], numeric_delta_b_1))\n",
    "    gradient_check(delta_b_1[k], numeric_delta_b_1)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.08964921]\n",
      " [-0.11842518]]\n"
     ]
    }
   ],
   "source": [
    "# 실제 학습이 잘 되는지 확인\n",
    "for _ in range(100): \n",
    "    train(i,t)\n",
    "query(i,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15315712, 0.74055256, 0.26306359, 0.53348796, 0.01495211,\n",
       "        0.91723843, 0.90297772],\n",
       "       [0.0324718 , 0.95742415, 0.13697192, 0.28359095, 0.60643929,\n",
       "        0.9428007 , 0.85487219],\n",
       "       [0.00186352, 0.52142388, 0.5519387 , 0.48527849, 0.76828255,\n",
       "        0.16012318, 0.76545081],\n",
       "       [0.01956525, 0.13583245, 0.11596188, 0.30958645, 0.67191935,\n",
       "        0.46936296, 0.81896852],\n",
       "       [0.28857552, 0.73363161, 0.70236954, 0.32731666, 0.33502675,\n",
       "        0.97654118, 0.62685746],\n",
       "       [0.94967597, 0.76779443, 0.82484986, 0.40648091, 0.45154749,\n",
       "        0.3996753 , 0.99657266]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 미분 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$b_{3} + w_{11} x_{1} + w_{12} x_{2} + w_{13} x_{3}$$"
      ],
      "text/plain": [
       "b₃ + w₁₁⋅x₁ + w₁₂⋅x₂ + w₁₃⋅x₃"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy\n",
    "sympy.init_printing(use_latex='mathjax')\n",
    "w11, x1, w12, x2, w13, x3, b3 = sympy.symbols('w11 x1 w12 x2 w13 x3 b3')\n",
    "z = w11 * x1 + w12 * x2 + w13 * x3 + b3\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$1$$"
      ],
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sympy.Derivative(z, b3).doit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
