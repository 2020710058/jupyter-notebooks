{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "7 x 5 x 2 의 3계층 신경망 구조를 작성하여 오차 역전파로 학습이 잘 진행되고, 편미분이 수치 미분의 결과와 동일한지 검증한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "def init():\n",
    "    global i, w_2, b_2, w_3, b_3, t\n",
    "        \n",
    "    i = np.array([0.4,-0.2,0.1,0.1,-0.15,0.6,-0.9]).reshape(-1, 1)\n",
    "\n",
    "    np.random.seed(12)\n",
    "    w_2 = np.random.rand(5, 7)\n",
    "    b_2 = np.random.rand(5).reshape(-1, 1)\n",
    "    w_3 = np.random.rand(2, 5)\n",
    "    b_3 = np.random.rand(2).reshape(-1, 1)\n",
    "    \n",
    "    t = np.array([[0.87503811],[0.83690408]])\n",
    "\n",
    "# 정답 행렬\n",
    "np.random.seed(10)\n",
    "t_w_2 = np.random.rand(5, 7)\n",
    "t_b_2 = np.random.rand(5).reshape(-1, 1)\n",
    "t_w_3 = np.random.rand(2, 5)\n",
    "t_b_3 = np.random.rand(2).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z: np.ndarray):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def d_sigmoid(z: np.ndarray):\n",
    "    return sigmoid(z) * (1.0 - sigmoid(z))\n",
    "\n",
    "def gradient_check(analytic, numeric):\n",
    "    numerator = abs(analytic-numeric)\n",
    "    denominator = max(analytic,numeric)\n",
    "    difference = numerator/denominator\n",
    "    \n",
    "    if difference < 1e-2: # cs231n의 권장 수치는 1e-7인데 여기서는 좀 더 높인다.\n",
    "        print (\"The gradient is correct!\")\n",
    "    else:\n",
    "        print (\"The gradient is wrong!\")\n",
    "        \n",
    "def forward(i):\n",
    "    global w_2, b_2, w_3, b_3\n",
    "    \n",
    "    net_h = np.dot(w_2, i) + b_2\n",
    "    out_h = sigmoid(net_h)\n",
    "    \n",
    "    net_o = np.dot(w_3, out_h) + b_3\n",
    "    out_o = sigmoid(net_o)\n",
    "    \n",
    "    return net_h, out_h, net_o, out_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(i, t):\n",
    "    lr = 0.1\n",
    "    \n",
    "    global w_2, w_3\n",
    "    \n",
    "    net_h, out_h, net_o, out_o = forward(i)\n",
    "\n",
    "    d_output_errors = - (t - out_o)\n",
    "    delta_w_3 = np.dot(d_output_errors * d_sigmoid(net_o), out_h.T)\n",
    "    w_3 += - lr * delta_w_3\n",
    "    \n",
    "    # backpropagation!\n",
    "    d_hidden_errors = np.dot(w_3.T, d_output_errors * d_sigmoid(net_o))\n",
    "    delta_w_2 = np.dot(d_hidden_errors * d_sigmoid(net_h), i.T)\n",
    "    w_2 += - lr * delta_w_2\n",
    "    \n",
    "    # bias 학습 필요\n",
    "    \n",
    "    return delta_w_2, delta_w_3\n",
    "\n",
    "def query(i, t):\n",
    "    _, _, _, out_o = forward(i)\n",
    "\n",
    "    # E 출력\n",
    "    print(t - out_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02447658]\n",
      " [-0.01159232]]\n"
     ]
    }
   ],
   "source": [
    "init()\n",
    "train(i,t)\n",
    "query(i,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0000551309011710, -0.0000550599404394\n",
      "The gradient is correct!\n",
      "-0.0002346868297633, -0.0002346188692002\n",
      "The gradient is correct!\n",
      "0.0000003325025091, 0.0000003956931354\n",
      "The gradient is wrong!\n",
      "-0.0002301421339975, -0.0002300861183437\n",
      "The gradient is correct!\n",
      "-0.0001182083089648, -0.0001181382365523\n",
      "The gradient is correct!\n",
      "\n",
      "0.0000275654505855, 0.0000275299732013\n",
      "The gradient is correct!\n",
      "0.0001173434148817, 0.0001173094580460\n",
      "The gradient is correct!\n",
      "-0.0000001662512546, -0.0000001978568676\n",
      "The gradient is correct!\n",
      "0.0001150710669987, 0.0001150430624245\n",
      "The gradient is correct!\n",
      "0.0000591041544824, 0.0000590691281695\n",
      "The gradient is correct!\n",
      "\n",
      "-0.0000137827252927, -0.0000137649966295\n",
      "The gradient is correct!\n",
      "-0.0000586717074408, -0.0000586547119468\n",
      "The gradient is correct!\n",
      "0.0000000831256273, 0.0000000989112221\n",
      "The gradient is wrong!\n",
      "-0.0000575355334994, -0.0000575215344648\n",
      "The gradient is correct!\n",
      "-0.0000295520772412, -0.0000295345738426\n",
      "The gradient is correct!\n",
      "\n",
      "-0.0000137827252927, -0.0000137649966295\n",
      "The gradient is correct!\n",
      "-0.0000586717074408, -0.0000586547119468\n",
      "The gradient is correct!\n",
      "0.0000000831256273, 0.0000000989112221\n",
      "The gradient is wrong!\n",
      "-0.0000575355334994, -0.0000575215344648\n",
      "The gradient is correct!\n",
      "-0.0000295520772412, -0.0000295345738426\n",
      "The gradient is correct!\n",
      "\n",
      "0.0000206740879391, 0.0000206474784102\n",
      "The gradient is correct!\n",
      "0.0000880075611613, 0.0000879820652097\n",
      "The gradient is correct!\n",
      "-0.0000001246884409, -0.0000001483774041\n",
      "The gradient is correct!\n",
      "0.0000863033002491, 0.0000862822977670\n",
      "The gradient is correct!\n",
      "0.0000443281158618, 0.0000443018508705\n",
      "The gradient is correct!\n",
      "\n",
      "-0.0000826963517565, -0.0000825899204170\n",
      "The gradient is correct!\n",
      "-0.0003520302446450, -0.0003519283071885\n",
      "The gradient is correct!\n",
      "0.0000004987537637, 0.0000005935562371\n",
      "The gradient is wrong!\n",
      "-0.0003452132009962, -0.0003451291870024\n",
      "The gradient is correct!\n",
      "-0.0001773124634472, -0.0001772073977900\n",
      "The gradient is correct!\n",
      "\n",
      "0.0001240445276347, 0.0001238848504033\n",
      "The gradient is correct!\n",
      "0.0005280453669675, 0.0005278924373369\n",
      "The gradient is correct!\n",
      "-0.0000007481306455, -0.0000008903107743\n",
      "The gradient is correct!\n",
      "0.0005178198014944, 0.0005176937437762\n",
      "The gradient is correct!\n",
      "0.0002659686951708, 0.0002658110735102\n",
      "The gradient is correct!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gradient Checking\n",
    "# w_2 검증\n",
    "\n",
    "init()\n",
    "delta_w_2, delta_w_3 = train(i,t)\n",
    "h = 1e-7\n",
    "\n",
    "init()\n",
    "for j in range(7):\n",
    "    for k in range(5):\n",
    "        # 수치 미분(numerical gradient) 진행\n",
    "        init()\n",
    "        w_2[k][j] += h\n",
    "        _, _, _, out_o = forward(i)\n",
    "        e1 = np.sum((t - out_o) ** 2) / 2\n",
    "\n",
    "        init()\n",
    "        w_2[k][j] -= h\n",
    "        _, _, _, out_o = forward(i)\n",
    "        e2 = np.sum((t - out_o) ** 2) / 2\n",
    "\n",
    "        # 수치 미분 결과가 해석적 미분(analytic gradient)과 동일한지 검증\n",
    "        numeric_delta_w_2 = (e1 - e2) / (2 * h)\n",
    "        print(\"%.16f, %.16f\" % (delta_w_2[k][j], numeric_delta_w_2))\n",
    "        gradient_check(delta_w_2[k][j], numeric_delta_w_2)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01847292]\n",
      " [-0.00897506]]\n"
     ]
    }
   ],
   "source": [
    "# 실제 학습이 잘 되는지 확인\n",
    "for _ in range(100): \n",
    "    train(i,t)\n",
    "query(i,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15464142, 0.73981041, 0.26343466, 0.53385904, 0.0143955 ,\n",
       "        0.91946488, 0.89963805],\n",
       "       [0.03543062, 0.95594474, 0.13771162, 0.28433065, 0.60532974,\n",
       "        0.94723892, 0.84821486],\n",
       "       [0.00226082, 0.52122523, 0.55203803, 0.48537781, 0.76813356,\n",
       "        0.16071913, 0.76455688],\n",
       "       [0.02277663, 0.13422676, 0.11676472, 0.31038929, 0.67071508,\n",
       "        0.47418002, 0.81174293],\n",
       "       [0.29062181, 0.73260847, 0.70288111, 0.32782823, 0.3342594 ,\n",
       "        0.97961061, 0.62225321]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습된 가중치와 정답 가중치 비교\n",
    "w_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77132064, 0.02075195, 0.63364823, 0.74880388, 0.49850701,\n",
       "        0.22479665, 0.19806286],\n",
       "       [0.76053071, 0.16911084, 0.08833981, 0.68535982, 0.95339335,\n",
       "        0.00394827, 0.51219226],\n",
       "       [0.81262096, 0.61252607, 0.72175532, 0.29187607, 0.91777412,\n",
       "        0.71457578, 0.54254437],\n",
       "       [0.14217005, 0.37334076, 0.67413362, 0.44183317, 0.43401399,\n",
       "        0.61776698, 0.51313824],\n",
       "       [0.65039718, 0.60103895, 0.8052232 , 0.52164715, 0.90864888,\n",
       "        0.31923609, 0.09045935]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_w_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 미분 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$b_{3} + w_{11} x_{1} + w_{12} x_{2} + w_{13} x_{3}$$"
      ],
      "text/plain": [
       "b₃ + w₁₁⋅x₁ + w₁₂⋅x₂ + w₁₃⋅x₃"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy\n",
    "sympy.init_printing(use_latex='mathjax')\n",
    "w11, x1, w12, x2, w13, x3, b3 = sympy.symbols('w11 x1 w12 x2 w13 x3 b3')\n",
    "z = w11 * x1 + w12 * x2 + w13 * x3 + b3\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$x_{1}$$"
      ],
      "text/plain": [
       "x₁"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sympy.Derivative(z, w11).doit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
